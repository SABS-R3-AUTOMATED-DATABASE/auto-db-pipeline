{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:paperscraper.load_dumps: No dump found for biorxiv. Skipping entry.\n",
      "WARNING:paperscraper.load_dumps: No dump found for chemrxiv. Skipping entry.\n",
      "WARNING:paperscraper.load_dumps: No dump found for medrxiv. Skipping entry.\n",
      "WARNING:paperscraper.load_dumps: No dumps found for either biorxiv or medrxiv. Consider using paperscraper.get_dumps.* to fetch the dumps.\n"
     ]
    }
   ],
   "source": [
    "import get_doi_from_pubmed as gd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = ['SARS-CoV-2', 'COVID-19', 'coronavirus', 'SARS-CoV', 'MERS-CoV',\n",
    "        'SARS']\n",
    "antibody = ['antibody', 'antibodies', 'nanobody', 'MAb', 'immunoglobulin',\n",
    "            'nanobodies']\n",
    "interaction = ['neutralizing', 'neutralize', 'neutralization', 'bind',\n",
    "               'binding', 'inhibit', 'targeting']\n",
    "extra = ['heavy chain',  'complementarity determining region',\n",
    "        'gene', 'epitope', 'receptor-binding domain', 'rbd',\n",
    "        'spike protein', 'VHH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_and_preprints = gd.pubmed_papers_and_pt(txt=False, jsonl=False, csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBChecker:\n",
    "    def __init__(self):\n",
    "        from Bio.PDB.PDBList import PDBList\n",
    "        \"\"\"\n",
    "        First we store all the existing pdb IDs as a dictionary for O(1) lookup. \n",
    "        There are 184,929 IDs as of 2021-12-8 and the retrieval using biopython takes about 7 seconds.\n",
    "        For some reason, calling PDBList() creates an empty folder in the directory called \"obsolete\", \n",
    "        but this goes away by setting the `obsolte_pdb` parameter to some random string, which I made \"None\".\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.pdbl = PDBList(verbose=False, obsolete_pdb=\"None\")\n",
    "        self.existing_pdbs = {pdb_id: True for pdb_id in self.pdbl.get_all_entries()}  # takes 7 secs\n",
    "\n",
    "    def get_actual(self, possible_pdbs: list, verbose=True) -> list:\n",
    "        \"\"\"\n",
    "        Takes a list of possible PDB IDs as input. \n",
    "        Returns a list of the actual PDB IDs, i.e. the ones from the input list that exist on the PDB database.\n",
    "        \n",
    "        \n",
    "        Warning: Please remember that html gobble can include actual PDB IDs by chance. So just because a possible\n",
    "        PDB ID from the paper url html turns out to be an actual PDB ID (is actually on the database), does not \n",
    "        mean it was meant to be written in the text of the paper. \n",
    "        \"\"\"\n",
    "        actual_pdbs = [pdb_id for pdb_id in possible_pdbs if self.existing_pdbs.get(pdb_id, False)]\n",
    "        if verbose: \n",
    "            print(\"Out of the\", len(actual_pdbs), 'possible PDB IDs scraped', len(possible_pdbs), 'are actual PDB IDs.')\n",
    "        return actual_pdbs\n",
    "        \n",
    "    def get_top_authors(self, pdb_id: str, top_num=3) -> list:\n",
    "        \"\"\"\n",
    "        Takes a \n",
    "        \"\"\"\n",
    "        \n",
    "        import tempfile\n",
    "        import re\n",
    "\n",
    "        temp_dir = tempfile.TemporaryDirectory()\n",
    "        pdb_file = self.pdbl.retrieve_pdb_file(pdb_id, file_format=\"pdb\", pdir=temp_dir.name)\n",
    "        author_txt = ' '.join(filter(lambda line: line.split()[0] == \"AUTHOR\", open(pdb_file).read().splitlines()))\n",
    "        temp_dir.cleanup()\n",
    "        authors = list(filter(lambda word: len(word) > 1 and word != \"AUTHOR\", re.findall(r\"[\\w']+\", author_txt)))\n",
    "        return authors[:top_num]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
